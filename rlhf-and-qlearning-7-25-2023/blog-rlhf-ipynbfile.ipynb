{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning and Q-Learning Blog\n",
        "Authored By Sai Siddhish Chandra Sekaran - July 25, 2023\n",
        "<br><br>\n",
        "Please read the full article at https://devsai.hashnode.dev/reinforcement-learning-and-qlearning as this is simply just the code."
      ],
      "metadata": {
        "id": "oZaN9MWrOAf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bash command to install the libraries\n",
        "!pip install numpy\n",
        "!pip install gymnasium[toy-text] # The Frozen Lake game is classified as a \"Toy Text\" game in Gym\n",
        "# If you are using the Python file please open your terminal and type in the commands above without the \"!\"\n",
        "\n",
        "# Python code to import the libraries we will use later\n",
        "import numpy as np\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "IDBSb7VdOi3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the environment\n",
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"ansi\", is_slippery=False)\n",
        "\n",
        "# Reset the environment to get a observation\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Printing the state\n",
        "# When the observation is printed, you will see that we recieve the number 0 like mentioned above\n",
        "print(observation)"
      ],
      "metadata": {
        "id": "Qy68nxVTPq6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.render())\n",
        "# S is the starting point\n",
        "# G is the goal\n",
        "# H's are holes\n",
        "# F's are the Frozen lake\n",
        "# The letter with a red/pink background is the character's current position"
      ],
      "metadata": {
        "id": "maldqCWhPzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will take a random action in the environment and see what we get in response\n",
        "\n",
        "# Selecting a random action\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# Executing that random action in the environment\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "print(f\"Observation: {observation}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Environment Terminated?: {terminated}\")\n",
        "print(f\"Environment Truncated?: {truncated}\")\n",
        "\n",
        "# Render the game\n",
        "print(\"\\n\" + env.render())\n",
        "\n",
        "# The character moving on it's own is simply random, next we will implement a RL model to make it play better!"
      ],
      "metadata": {
        "id": "S1JQ0qm9Px-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our Q-Table to all 0 values (the RL agent bases it's decisions off of this table and updates it in training)\n",
        "QTable = np.zeros((env.observation_space.n, env.action_space.n))"
      ],
      "metadata": {
        "id": "VwKzQQPLP1Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy Epsilon Function, this is what determines what action we will take\n",
        "def greedyEpsilon(Qtable, state, epsilon):\n",
        "  num = np.random.rand()\n",
        "  if (num < epsilon):\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = np.argmax(Qtable[state])\n",
        "  return action"
      ],
      "metadata": {
        "id": "zjJ4MgSbP6Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will train our agent (this might take a few minutes)\n",
        "def train(env, QTable, numOfEpisodes, learningRate, discountFactor, startingEpsilon, finalEpsilon, decayRate, maxSteps):\n",
        "  for i in range(numOfEpisodes):\n",
        "    # Decaying Epsilon so that we get more exploitation than exploration (over time)\n",
        "    epsilon = startingEpsilon + (finalEpsilon - startingEpsilon) * np.exp(-decayRate * i)\n",
        "\n",
        "    # Reset the environment and get an observation\n",
        "    currentState = env.reset()\n",
        "    currentState = currentState[0]\n",
        "\n",
        "    for j in range(maxSteps):\n",
        "      # Determine what action to take with the Greedy Epsilon function\n",
        "      action = greedyEpsilon(QTable, currentState, epsilon)\n",
        "\n",
        "      # Retrieve important info from environment and apply our action in the environment\n",
        "      newState, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update the Q Table to reflect what the agent has learned\n",
        "      QTable[currentState][action] = (1 - learningRate) * QTable[currentState][action] + learningRate * (reward + discountFactor * np.max(QTable[newState]))\n",
        "\n",
        "      # If the game is terminated or truncated, finish this session (this \"session\" is often called an \"episode\")\n",
        "      if (terminated or truncated):\n",
        "        break\n",
        "\n",
        "      # Update the state we were basing everything off of, to the new state\n",
        "      currentState = newState\n",
        "  return QTable\n",
        "\n",
        "train(env, QTable, 10000, 0.5, 0.97, 0.01, 1, 0.0005, 10000)"
      ],
      "metadata": {
        "id": "QoZtyEIUP6_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see how well our agent performs when playing the game\n",
        "\n",
        "# Variables\n",
        "numOfWins = 0\n",
        "numOfLosses = 0\n",
        "for i in range(100):\n",
        "  # Get the current state/observation\n",
        "  state = env.reset()\n",
        "  state = state[0]\n",
        "  for j in range(1000):\n",
        "    # Get the action from the QTable with the highest reward\n",
        "    action = np.argmax(QTable[state][:])\n",
        "\n",
        "    # Retrieve important info from environment and apply our action in the environment\n",
        "    newState, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # If the reward equals 1 (we got to the goal) add one to the number of wins\n",
        "    # Else add one to the number of losses\n",
        "    if (reward == 1):\n",
        "      numOfWins += 1\n",
        "    else:\n",
        "      numOfLosses += 1\n",
        "\n",
        "    # If the game is terminated or truncated, finish this session (this \"session\" is often called an \"episode\")\n",
        "    if (terminated or truncated):\n",
        "        break\n",
        "\n",
        "    # Update the state we were basing everything off of, to the new state\n",
        "    state = newState\n",
        "\n",
        "print(f\"Number of wins: {numOfWins}\")\n",
        "print(f\"Number of losses: {numOfLosses}\")"
      ],
      "metadata": {
        "id": "w6ADHo4JP8nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for reading the article and running this code!<br>\n",
        "Also consider [following me over on Hashnode](https://hashnode.com/@Cakez). All the support is appreciated"
      ],
      "metadata": {
        "id": "4JS1S6fwQBtu"
      }
    }
  ]
}